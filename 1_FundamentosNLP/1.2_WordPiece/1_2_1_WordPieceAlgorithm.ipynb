{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPg9om9qQCFt+0Ezau62kFP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juacardonahe/Curso_NLP/blob/main/1_FundamentosNLP/1.2_WordPiece/1_2_1_WordPieceAlgorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/juacardonahe/Curso_NLP/refs/heads/main/data/UnFieldB.png\" width=\"40%\">\n",
        "\n",
        "# **Procesamiento de Lenguaje Natural (NLP)**\n",
        "### Departamento de Ingeniería Eléctrica, Electrónica y Computación\n",
        "#### Universidad Nacional de Colombia - Sede Manizales\n",
        "\n",
        "#### Elaboró: Juan José Cardona H.\n",
        "#### Revisó: Diego A. Perez"
      ],
      "metadata": {
        "id": "n0Nj7H_4O5Jy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1.2.1 - WordPiece Algorithm**"
      ],
      "metadata": {
        "id": "3tRdFAPvZXt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WordPiece** is the tokenization algorithm that Google developed to pretrain BERT. It has since been reused in several BERT-based Transformer models, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It is very similar to BPE in terms of training, but the tokenization itself is done differently.\n",
        "\n",
        "⚠️ *Google has never released the source code for its implementation of the WordPiece training algorithm.*"
      ],
      "metadata": {
        "id": "C8gj_Hikccgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like ## for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, \"word\" gets split like this:\n",
        "\n",
        "```\n",
        "w ##o ##r ##d\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "U7WPmDxoeRKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The WordPiece algorithm is iterative and the summary of the algorithm according to the original paper [\"Japanese and Korean Voice Search (Schuster et al., 2012)\"](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf), is as follows:\n",
        "\n",
        "1. Initialize the word unit inventory with the base characters.\n",
        "\n",
        "2. Build a language model on the training data using the word inventory from 1.\n",
        "\n",
        "3. Generate a new word unit by combining two units out of the current word inventory. The word unit inventory will be incremented by 1 after adding this new word unit. The new word unit is chosen from all the possible ones so that it increases the likelihood of the training data the most when added to the model.\n",
        "\n",
        "4. Goto 2 until a pre-defined limit of word units is reached or the likelihood increase falls below a certain threshold."
      ],
      "metadata": {
        "id": "V-SajsF4eZCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Implementing WordPiece Algorithm**"
      ],
      "metadata": {
        "id": "1rjeAneBgvpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Since we're replicating a WordPiece tokenizer (like BERT), we'll use the `bert-base-cased` tokenizer for pretokenization:"
      ],
      "metadata": {
        "id": "q8ozVUKvhqTj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QCqKveQOJ_YF"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the tokenizer is loaded\n",
        "print('Tokenizer loaded successfully! Vocabulary size:', len(tokenizer.vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOozrSRPlSjc",
        "outputId": "b33a4f15-cc3a-4739-bca8-6ceccead3cbf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded successfully! Vocabulary size: 30522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vocabulary size tells us how many unique tokens or subwords the tokenizer knows."
      ],
      "metadata": {
        "id": "8KsRhDGolK6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**1. Tokenization example**\n",
        "Let's tokenize a sample sentence to see how WordPiece breaks down words into subword units. The WordPiece tokenizer splits unknown words into smaller known pieces from its vocabulary."
      ],
      "metadata": {
        "id": "6HaQ9LG5oSZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "# Convert tokens to IDs\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "# Print results\n",
        "print('Original sentence:', sentence)\n",
        "print('Tokens:', tokens)\n",
        "print('Token IDs:', token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lQnbRzAllQ8",
        "outputId": "3b9a3443-fdcb-4d94-a6b1-4ef546a9ecb5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence: The quick brown fox jumps over the lazy dog\n",
            "Tokens: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
            "Token IDs: [1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try a more complex word to see how WordPiece handles it. We'll use a word like 'playing' to observe subword units."
      ],
      "metadata": {
        "id": "IhjC3bzNlsQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complex word example\n",
        "word = \"playing\"\n",
        "\n",
        "# Tokenize the word\n",
        "tokens = tokenizer.tokenize(word)\n",
        "\n",
        "# Convert tokens to IDs\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "# Print results\n",
        "print('Original word:', word)\n",
        "print('Tokens:', tokens)\n",
        "print('Token IDs:', token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TgDwIKLne4g",
        "outputId": "343aa088-a0cd-4821-b2cb-d0a2ece27949"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original word: playing\n",
            "Tokens: ['playing']\n",
            "Token IDs: [2652]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The WordPiece tokenizer often breaks words with affixes (like \"-ing\") into smaller units. For \"playing,\" it typically splits into two tokens: \"play\" and \"##ing.\" The \"##\" indicates that \"##ing\" is a suffix continuing the previous token. This shows how WordPiece handles morphological components (root + suffix) rather than keeping the word intact."
      ],
      "metadata": {
        "id": "sZSmqTtGoALC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**2. IDs to text**\n",
        "We can convert the token IDs back to text to see how the tokenizer reconstructs the input."
      ],
      "metadata": {
        "id": "0BmU8BOPocor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode token IDs back to text\n",
        "decoded_text = tokenizer.decode(token_ids)\n",
        "\n",
        "print('Token IDs:', token_ids)\n",
        "print('Decoded text:', decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7nkCYpdoBl7",
        "outputId": "6d56d357-d872-4272-ed41-4bf3b3419958"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: [2652]\n",
            "Decoded text: playing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**3. Handle special tokens**\n",
        "The BERT tokenizer adds special tokens like `[CLS]` and `[SEP]` when encoding sentences for model input. Let's see how this works."
      ],
      "metadata": {
        "id": "nuYGd9U_omv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the sentence with special tokens\n",
        "encoded_input = tokenizer(sentence, add_special_tokens=True, return_tensors='pt')\n",
        "\n",
        "# Decode to see special tokens\n",
        "decoded_with_special = tokenizer.decode(encoded_input['input_ids'][0])\n",
        "\n",
        "print('Encoded input IDs:', encoded_input['input_ids'])\n",
        "print('Decoded with special tokens:', decoded_with_special)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mG4hev4TopkO",
        "outputId": "a8ebbf20-2a82-4971-8078-96e00a1d571f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded input IDs: tensor([[  101,  1996,  4248,  2829,  4419, 14523,  2058,  1996, 13971,  3899,\n",
            "           102]])\n",
            "Decoded with special tokens: [CLS] the quick brown fox jumps over the lazy dog [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Summary**\n",
        "WordPiece algorithm trains a language model on the base vocabulary, picks the pair which has the highest likelihood, add this pair to the vocabulary, train the language model on the new vocabulary and repeat the steps repeated until the desired vocabulary size or likelihood threshold is reached."
      ],
      "metadata": {
        "id": "n_w1ePhbpNLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Notes**\n",
        "- The WordPiece tokenizer splits words into subword units (e.g., 'playing' → 'play' + '##ing'). The '##' indicates a subword that continues a previous word.\n",
        "- The `bert-base-uncased` tokenizer converts all text to lowercase before tokenizing.\n",
        "- Special tokens like `[CLS]` and `[SEP]` are used by BERT for specific tasks, such as classification or separating sentences.\n",
        "\n",
        "You can experiment by changing the `sentence` or `word` variables to see how different inputs are tokenized!"
      ],
      "metadata": {
        "id": "BxwhKLNSo1wy"
      }
    }
  ]
}