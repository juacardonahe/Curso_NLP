{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNXSBJB05HHSPFaP78itEIN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juacardonahe/Curso_NLP/blob/main/1_FundamentosNLP/1.1_Introducci%C3%B3nNLP/1_1_5_PreProcesing_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/juacardonahe/Curso_NLP/refs/heads/main/data/UNAL_Field_w.png\" width=\"40%\">\n",
        "\n",
        "# **Procesamiento de Lenguaje Natural (NLP)**\n",
        "### Departamento de Ingeniería Eléctrica, Electrónica y Computación\n",
        "#### Universidad Nacional de Colombia - Sede Manizales\n",
        "\n",
        "#### Elaboró: Juan José Cardona H.\n",
        "#### Revisó: Diego A. Perez"
      ],
      "metadata": {
        "id": "Of_A4nOgb3AM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.1.5 - Data Pre-Procesing**\n",
        "\n",
        "Los modelos de NLP funcionan al identificar relaciones entre las partes que componen el lenguaje, por ejemplo, las letras, palabras y oraciones presentes en un conjunto de datos de texto. Las arquitecturas de NLP utilizan diversos métodos para el preprocesamiento de datos, la extracción de características y el modelado.\n",
        "\n",
        "En este notebook vamos a simplificar y desglosar las tecnicas de pre-procesamiento de datos para una mejor comprensión."
      ],
      "metadata": {
        "id": "vdOKjToOcbem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Instalación e importación de librerías**"
      ],
      "metadata": {
        "id": "a8u49ipXgpv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instalación\n",
        "!pip install nltk\n",
        "\n",
        "#Importación\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6biakAyguir",
        "outputId": "e898dc88-2f22-4794-fc0d-1d799858c74b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenization**"
      ],
      "metadata": {
        "id": "Z-QnPoC0d7yu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El proceso de tokenización consiste en dividir un texto en unidades más pequeñas llamadas tokens, que pueden ser palabras, subpalabras o caracteres.\n",
        "\n",
        "Tipos:\n",
        "\n",
        "1. **Tokenización de palabras (Word Tokenization)**: Consiste en dividir el texto en palabras individuales.\n",
        "   Ejemplo:\n",
        "   “The quick brown fox jumps over the lazy dog.” se tokeniza como:\n",
        "   `[‘The’, ‘quick’, ‘brown’, ‘fox’, ‘jumps’, ‘over’, ‘the’, ‘lazy’, ‘dog’, ‘.’]`\n",
        "\n",
        "2. **Tokenización de oraciones (Sentence Tokenization)**: Consiste en dividir el texto en oraciones individuales.\n",
        "   Ejemplo:\n",
        "   “I love programming. It’s an amazing skill to have.” se tokeniza como:\n",
        "   `[‘I love programming.’, ‘It’s an amazing skill to have.’]`\n",
        "\n",
        "3. **Tokenización de subpalabras**: Se divide las palabras en unidades más pequeñas como prefijos, sufijos o caracteres individuales.\n"
      ],
      "metadata": {
        "id": "UGlS4hlkouBA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnkEZ6uOby7h",
        "outputId": "8e6e82d0-fef7-4b3e-fc70-83c4bea2b777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenización de palabras:\n",
            "['I', 'love', 'programming', '.', 'It', '’', 's', 'an', 'amazing', 'skill', 'to', 'have', '.', 'Tokenization', 'is', 'an', 'essential', 'step', 'in', 'NLP', '.']\n",
            "\n",
            "Tokenización de oraciones:\n",
            "['I love programming.', 'It’s an amazing skill to have.', 'Tokenization is an essential step in NLP.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"I love programming. It’s an amazing skill to have. Tokenization is an essential step in NLP.\"\n",
        "\n",
        "# word tokenization\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Tokenización de palabras:\")\n",
        "print(word_tokens)\n",
        "\n",
        "# sentence tokenization\n",
        "sentence_tokens = sent_tokenize(text)\n",
        "print(\"\\nTokenización de oraciones:\")\n",
        "print(sentence_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "La tokenización es el primer paso en muchos flujos de trabajo de NLP y tiene un impacto directo en las etapas de procesamiento posteriores."
      ],
      "metadata": {
        "id": "TGTyXwEBpUoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stemming**"
      ],
      "metadata": {
        "id": "WaTJaxJJeA8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El stemming es el proceso de reducir las palabras a su forma raíz, eliminando sufijos y prefijos.\n",
        "\n",
        "*Ejemplo: Las palabras “running” y “runner” se reducen a “run”.*"
      ],
      "metadata": {
        "id": "7EWu5Pv8prif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = [\"running\", \"runner\", \"easily\", \"flies\", \"better\", \"happily\"]\n",
        "\n",
        "#Stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Palabras originales:\", words)\n",
        "print(\"Palabras con stemming:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsdgHkAneDnI",
        "outputId": "9a30882e-d972-41ea-a9a6-f6740b6438b1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras originales: ['running', 'runner', 'easily', 'flies', 'better', 'happily']\n",
            "Palabras con stemming: ['run', 'runner', 'easili', 'fli', 'better', 'happili']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El stemming es una técnica más rudimentaria en comparación con la lematización, y no siempre produce palabras válidas del idioma, ya que no tiene en cuenta el contexto gramatical de la palabra."
      ],
      "metadata": {
        "id": "xV2UCmj-p21c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lemmatization**"
      ],
      "metadata": {
        "id": "xqxpTkrbeE_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La lematización es el proceso de reducir una palabra a su forma base o de diccionario, conocida como lema.\n",
        "\n",
        "*Ejemplo: Las palabras \"running\" y \"ran\" se lematizan a \"run\".*\n",
        "\n",
        "Su importancia recae en que nos permite agrupar distintas formas de una palabra bajo un mismo concepto, facilitando la comprensión del significado subyacente en los textos.\n",
        "\n",
        "**Nota**\n",
        "\n",
        "Los algoritmos de stemming son más rápidos y requieren menos recursos computacionales que los procesos de lematización, aunque suelen ser menos precisos.\n"
      ],
      "metadata": {
        "id": "cY10Cz2onRgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "# create an object of class WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(lemmatizer.lemmatize(\"eats\", 'v'))\n",
        "print(lemmatizer.lemmatize(\"ate\", 'v'))\n",
        "print(lemmatizer.lemmatize(\"eat\", 'v'))\n",
        "print(lemmatizer.lemmatize(\"eating\", 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9BxYqifeIiR",
        "outputId": "bfb1e53b-b94a-4460-96ee-88abe35537cc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eat\n",
            "eat\n",
            "eat\n",
            "eat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La lematización consiste en agrupar las diferentes formas flexionadas de una misma palabra. De este modo, se puede acceder a la forma base de cada término, la cual conserva un significado claro y coherente. Esta forma base se denomina **lema**.\n"
      ],
      "metadata": {
        "id": "OASzCYoToGM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los procesos de lematización suelen ser más lentos y computacionalmente más costosos que los algoritmos de stemming. Esto se debe a que la lematización implica un análisis más profundo del significado y la categoría gramatical de cada palabra, buscando su forma base correcta en un diccionario, mientras que el stemming aplica reglas más simples sin considerar el contexto.\n"
      ],
      "metadata": {
        "id": "--Zbvtd7n_YR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Normalization**"
      ],
      "metadata": {
        "id": "Riv24ETteJAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La normalización en el Procesamiento de Lenguaje Natural (NLP) se refiere al conjunto de procedimientos destinados a transformar el texto en una forma estandarizada. El objetivo principal de esta etapa es garantizar la coherencia en el tratamiento de los datos textuales, facilitando así una mejor comprensión e interpretación por parte de los modelos de NLP. Entre las técnicas de normalización más comunes se encuentran la conversión de todo el texto a minúsculas, la eliminación de signos de puntuación y palabras vacías (stopwords), así como la aplicación de procesos de reducción como el stemming y la lematización.\n"
      ],
      "metadata": {
        "id": "lD-8iaNTl8FB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Técnicas de normalización**\n",
        "\n",
        "###**Conversión a minúsculas**\n",
        "\n",
        "Este proceso transforma todos los caracteres del texto en minúsculas, con el fin de tratar palabras que solo difieren en el uso de mayúsculas como equivalentes, optimizando así el análisis de los datos textuales.\n",
        "\n",
        "*Ejemplo: \"House\" → \"house\"*\n",
        "\n",
        "### **Eliminación de signos de puntuación**\n",
        "\n",
        "La eliminación de signos de puntuación es una práctica común en PLN, ya que estos símbolos suelen no aportar un significado relevante para el análisis semántico del texto.\n",
        "\n",
        "*Ejemplo: \"Welcome, friend!\" → \"Welcome friend\"*\n",
        "\n",
        "### **Eliminación de palabras vacías**\n",
        "\n",
        "Las palabras vacías (stop words) son términos muy frecuentes en el lenguaje natural (como \"and\", \"the\", \"in\") que suelen ser descartados porque su contribución semántica al contenido principal es limitada.\n",
        "\n",
        "*Ejemplo: \"She is reading a book\" → \"reading book\"*\n"
      ],
      "metadata": {
        "id": "kotrZcyomFVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "text = \"Learning new languages opens many opportunities around the world.\"\n",
        "\n",
        "#Convierte el texto a minusculas\n",
        "text_lower = text.lower()\n",
        "print(\"Lowercased text:\", text_lower)\n",
        "\n",
        "#Elimina puntuación\n",
        "text_no_punct = re.sub(r'[^\\w\\s]', '', text_lower)\n",
        "print(\"Text without punctuation:\", text_no_punct)\n",
        "\n",
        "#Tokenization\n",
        "words = nltk.word_tokenize(text_no_punct)\n",
        "print(\"Tokenized words:\", words)\n",
        "\n",
        "#Elimina Stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words_no_stop = [word for word in words if word not in stop_words]\n",
        "print(\"Text without stopwords:\", words_no_stop)\n",
        "\n",
        "#Stemming\n",
        "ps = PorterStemmer()\n",
        "words_stemmed = [ps.stem(word) for word in words_no_stop]\n",
        "print(\"Stemmed words:\", words_stemmed)\n",
        "\n",
        "#Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words_lemmatized = [lemmatizer.lemmatize(word) for word in words_no_stop]\n",
        "print(\"Lemmatized words:\", words_lemmatized)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTo5DzRjeL9O",
        "outputId": "49db5131-2c23-41b1-b3cf-062726e85cc5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercased text: learning new languages opens many opportunities around the world.\n",
            "Text without punctuation: learning new languages opens many opportunities around the world\n",
            "Tokenized words: ['learning', 'new', 'languages', 'opens', 'many', 'opportunities', 'around', 'the', 'world']\n",
            "Text without stopwords: ['learning', 'new', 'languages', 'opens', 'many', 'opportunities', 'around', 'world']\n",
            "Stemmed words: ['learn', 'new', 'languag', 'open', 'mani', 'opportun', 'around', 'world']\n",
            "Lemmatized words: ['learning', 'new', 'language', 'open', 'many', 'opportunity', 'around', 'world']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part of Speech (POS) Tagging**"
      ],
      "metadata": {
        "id": "yIzaNZ3deMPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El etiquetado de partes del discurso, conocido como POS Tagging, es el proceso de asignar a cada palabra en una oración una categoría gramatical específica, como sustantivos, verbos, adjetivos, adverbios, pronombres, entre otros. Este proceso se realiza analizando el contexto en el que aparece cada palabra dentro de la oración.\n",
        "\n",
        "Su **importancia** recae en que nos ayuda a comprender la estructura gramatical y el significado de las oraciones."
      ],
      "metadata": {
        "id": "7deE-KrmeZAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos ahora como NLTK’s POS tagger nos etiqueta la siguiente frase:\n",
        "\n",
        "```\n",
        "\"The quick brown fox jumps over the lazy dog.\"\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "myJg5TPwhmWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag  # importa el etiquetador POS\n",
        "from nltk import word_tokenize  # importa la función para tokenizar texto\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "tokenized_text = word_tokenize(text)  # tokeniza el texto\n",
        "\n",
        "tags = pos_tag(tokenized_text)  # etiqueta las palabras con POS\n",
        "\n",
        "tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icVMcVXPeU5h",
        "outputId": "4d97623a-c941-4fc5-e09f-e6ed2304ff8f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('quick', 'JJ'),\n",
              " ('brown', 'NN'),\n",
              " ('fox', 'NN'),\n",
              " ('jumps', 'VBZ'),\n",
              " ('over', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('lazy', 'JJ'),\n",
              " ('dog', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El código anterior utiliza el etiquetador de partes de la oración (POS tagger) de NLTK para analizar la siguiente frase:\n",
        "\n",
        "\"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "El etiquetado POS asigna etiquetas a cada palabra en el texto para indicar su categoría gramatical. Las etiquetas de POS que se utilizan son las siguientes:\n",
        "\n",
        "- **DT** (Determiner): Determinante. \"The\" es un artículo definido, que determina el sustantivo \"fox\" y \"dog\".\n",
        "  \n",
        "- **JJ** (Adjective): Adjetivo. \"quick\" y \"brown\" son adjetivos que describen a \"fox\".\n",
        "  \n",
        "- **NN** (Noun, Singular or Mass): Sustantivo común en singular. \"fox\" y \"dog\" son sustantivos que se refieren a animales.\n",
        "  \n",
        "- **VBZ** (Verb, 3rd Person Singular, Present): Verbo en tercera persona singular del presente. \"jumps\" es el verbo en esta oración.\n",
        "\n",
        "- **IN** (Preposition): Preposición. \"over\" indica la relación espacial entre \"fox\" y \"dog\".\n"
      ],
      "metadata": {
        "id": "-bW-Ghxzfqj9"
      }
    }
  ]
}